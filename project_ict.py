# -*- coding: utf-8 -*-
"""project_ICT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10ilcLREGukR4xn0eCTLTRoxS-NdNaLd0

### Target 1
"""

#importing the necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

#loading the data to read
data=pd.read_csv("cardio_train.csv")

data

data.shape

#converting the age (in days) to age (in yrs)
data['age'] = data['age']/365

data.describe()

data.isna().sum()

data.dtypes

#storing the age values as int
data['age'] = data['age'].astype(int)

data

data.age.nunique()

data.columns

gender = np.array(data['gender'].unique())

ob_value_count= data['gender'].value_counts()['male']

ob_value_count

#extracting the count of females
ob_value_count2 = data['gender'].value_counts()['female']

ob_value_count2

"""# **Pie Chart**"""

df = pd.DataFrame(data)

#extracting the count of males and females who has cardio disease
male_count = df[(df['gender'] == 'male') & (df['cardio'] == 1)]
female_count = df[(df['gender'] == 'female') & (df['cardio'] == 1)]

male_count.nunique()

female_count.nunique()

#extracting the count of males and females who doesnot has a cardio disease
male_count1 = df[(df['gender'] == 'male') & (df['cardio'] == 0)]
female_count1 = df[(df['gender'] == 'female') & (df['cardio'] == 0)]

male_count1.nunique()

female_count1.nunique()

#plotting the piechart with the extracted data
gender_data = {
    'male': 24470,
    'female': 45530,
}

cardio_data = {
    'cardio': 34979,
    'non-cardio': 35021,
}

# Plotting the pie chart for gender data
plt.subplot(1, 2, 1)  # Creating a subplot for gender data
plt.pie(gender_data.values(), labels=gender_data.values(), autopct='%1.1f%%')
plt.title('Gender Distribution')



plt.show()

#extracting the count of both males and females who suffer from cardio disease as well as those who doesnot has the cardio disease
cardio_p=[len(data[(data['gender'] == 'male') & (data['cardio'] == 1)]),len(data[(data['gender'] == 'female') & (data['cardio'] == 1)])]
cardio_n=[len(data[(data['gender'] == 'male') & (data['cardio'] == 0)]),len(data[(data['gender'] == 'female') & (data['cardio'] == 0)])]

#plotting the piechart for the data of the males and females who suffer from cardio disease
plt.pie(cardio_p ,labels=gender, autopct='%1.1f%%')
plt.title('Males and Females with cardio disease')
plt.show()

#plotting the piechart for the data of males and females who doesnot suffer from  cardio disease
plt.pie(cardio_n ,labels=gender, autopct='%1.1f%%')
plt.title('Males and Females with non-cardio disease')
plt.show()

#extracting the count of males and females those who smokes
male_count2 = df[(df['gender'] == 'male') & (df['smoke'] == 'yes')] 
female_count2 = df[(df['gender'] == 'female') & (df['smoke'] == 'yes')]

male_count2.nunique()

female_count2.nunique()

#extracting the count of males and females who donot smokes
male_count2 = df[(df['gender'] == 'male') & (df['smoke'] == 'no')]
female_count2 = df[(df['gender'] == 'female') & (df['smoke'] == 'no')]

male_count2.nunique()

female_count2.nunique()

smo=[len(data[(data['gender'] == 'male') & (data['smoke'] == 'yes')]),len(data[(data['gender'] == 'female') & (data['smoke'] == 'yes')])]
smok=[len(data[(data['gender'] == 'male') & (data['smoke'] == 'no')]),len(data[(data['gender'] == 'female') & (data['smoke'] == 'no')])]

#plotting pie with the collected data
plt.pie(smo,labels=gender, autopct='%1.1f%%')
plt.title('Male and Females who smokes')
plt.show()

plt.pie(smok ,labels=gender, autopct='%1.1f%%')
plt.title('Male and Females who donot smokes')
plt.show()

#extracting the count of the people who smokes and doesnot smokes has the cardio as well as non_cardio disease
smoke_yn=[len(data[(data['smoke'] == 'yes') & (data['cardio'] == 1)]),len(data[(data['smoke'] == 'no') & (data['cardio'] == 1)])]
smoke_ny=[len(data[(data['smoke'] == 'yes') & (data['cardio'] == 0)]),len(data[(data['smoke'] == 'no') & (data['cardio'] == 0)])]
smoke= ['smokes','non-smokes']

#plotted the pie for the data
plt.pie(smoke_yn ,labels=smoke, autopct='%1.1f%%')
plt.title('Distribution of smoker and non-smoker having cardio disease')
plt.show()

plt.pie(smoke_ny ,labels=smoke, autopct='%1.1f%%')
plt.title('Distribution of smoker and non-smoker doesnot have cardio disease')
plt.show()

#extracting the count of active and non-active people who has cardio as well as non-cardio disease
act=['active','non-active']
active_p=[len(data[(data['active'] == 'active') & (data['cardio'] == 1)]),len(data[(data['active'] == 'not active') & (data['cardio'] == 1)])]
active_n=[len(data[(data['active'] == 'active') & (data['cardio'] == 0)]),len(data[(data['active'] == 'not active') & (data['cardio'] == 0)])]

#plotting the pie for the collected data
plt.pie(active_p ,labels=act, autopct='%1.1f%%')
plt.title('Distribution of active and non-active having cardio disease')
plt.show()

plt.pie(active_n ,labels=act, autopct='%1.1f%%')
plt.title('Distribution of active and non-active doesnot have cardio disease')
plt.show()

"""# ***Scatter Plot***"""

plt.figure(figsize=(6,4))
plt.scatter(data['gender'],data['ap_hi'],s=10)
plt.title('Plot of Gender vs Systolic blood pressure',fontsize=16)
plt.xlabel('gender')
plt.ylabel('ap_hi')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['gender'],data['ap_lo'],s=10)
plt.title('Plot of Gender vs Diastolic blood pressure',fontsize=16)
plt.xlabel('gender')
plt.ylabel('ap_lo')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['ap_hi'],data['ap_lo'],s=10)
plt.title('Plot of Systolic BP vs Diastolic BP',fontsize=16)
plt.xlabel('ap_lo')
plt.ylabel('ap_hi')
plt.show()

plt.scatter(data.age,y=data['height'])

plt.scatter(data.age,y=data['weight'])

plt.figure(figsize=(6,4))
plt.scatter(data['height'],data['weight'],s=10)
plt.title('Plot of Height vs Weight',fontsize=16)
plt.xlabel('height')
plt.ylabel('weight')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['gender'],data['height'],s=10)
plt.title('Plot of Gender vs Height',fontsize=16)
plt.xlabel('gender')
plt.ylabel('height')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['gender'],data['weight'],s=10)
plt.title('Plot of Gender vs weight',fontsize=16)
plt.xlabel('gender')
plt.ylabel('weight')
plt.show()

data.columns

"""# **Box Plot**"""

data.columns

for i in [ 'ap_hi', 'weight','height','age','ap_lo']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""**Correlation**"""

df = pd.DataFrame(data)

# Perform label encoding for the 'Gender' column
df['gender'] = df['gender'].map({'male': 0, 'female': 1})
df = pd.get_dummies(df, columns=['cholesterol'])
df = pd.get_dummies(df, columns=['gluc'])
df['smoke'] = df['smoke'].map({'yes': 1, 'no': 0})
df['alco'] = df['alco'].map({'yes': 1, 'no': 0})
# Convert 'Physical Activity' column to numeric
df['active'] = df['active'].map({'active': 1, 'not active': 0})

# Calculate correlation matrix
correlation_matrix = df.corr()

# Display correlation matrix   { (1)--strong positive correlation,  values close to (-1)--strong negative correlation, and values close to (0)--weak correlation}
print(correlation_matrix)

#  columns for the heatmap
heatmap_cols = ['age', 'height', 'weight', 'ap_lo', 'ap_hi', 'cardio']
heatmap_data = data[heatmap_cols]
# Create heatmap using seaborn
sns.heatmap(heatmap_data.corr(), cmap='coolwarm', annot=True)
plt.title('Correlation Heatmap')
plt.show()

"""### Target 2

# **Encoding**
"""

from sklearn.preprocessing import LabelEncoder

# Create an instance of LabelEncoder
label_encoder = LabelEncoder()

# Fit the encoder on the gender column
label_encoder.fit(data['gender'])

# Transform the gender column with label encoding
data['gender'] = label_encoder.transform(data['gender'])
label_encoder.fit(data['cholesterol'])

# Transform the gender column with label encoding
data['cholesterol'] = label_encoder.transform(data['cholesterol'])

label_encoder.fit(data['gluc'])

# Transform the gender column with label encoding
data['gluc'] = label_encoder.transform(data['gluc'])

label_encoder.fit(data['active'])

# Transform the gender column with label encoding
data['active'] = label_encoder.transform(data['active'])

label_encoder.fit(data['smoke'])

# Transform the gender column with label encoding
data['smoke'] = label_encoder.transform(data['smoke'])

label_encoder.fit(data['alco'])

# Transform the gender column with label encoding
data['alco'] = label_encoder.transform(data['alco'])

data.head()

"""# **OutLier Detection**

# **Systolic BP**
"""

Q1 = np.percentile(data['ap_hi'],25,interpolation ='midpoint')
Q2 = np.percentile(data['ap_hi'],50,interpolation ='midpoint')
Q3 = np.percentile(data['ap_hi'],75,interpolation ='midpoint')

print(Q1)
print(Q2)
print(Q3)

IQR = Q3-Q1
print(IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR

low_lim

up_lim

#notrequired

outlier=[]
for x in data['ap_hi']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)

outlier

#notrequired

ind1 = data['ap_hi'] < low_lim
result = data.loc[ind1].index.tolist()

#notrequired

result

#notrequired

data.drop([209, 383, 567, 636, 927, 979, 1600, 1627, 1772, 2167, 2203, 2431, 2528, 2612, 2853, 2990, 3447, 3449, 3623, 3683, 3834, 3846, 3858, 3905, 3949, 4280, 4347, 4382, 4465, 4582, 4595, 4607, 4616, 4685, 4830, 4941, 4983, 5225, 5382, 5712, 5760, 6569, 7076, 7374, 7506, 7710, 7949, 8141, 8280, 8349, 8422, 8757, 8947, 9285, 9298, 9452, 9735, 9897, 10232, 10723, 11050, 11060, 11102, 11146, 11454, 11577, 11793, 11951, 13172, 13675, 13709, 13755, 13883, 14127, 14269, 14453, 14500, 14755, 15704, 16021, 16340, 16479, 16629, 17248, 17277, 17328, 17529, 17872, 17929, 17939, 18096, 18514, 18877, 19044, 19363, 19457, 20192, 20536, 20694, 20697, 20777, 20886, 20917, 20954, 21425, 22075, 22510, 22670, 22772, 22826, 23703, 23988, 24431, 24537, 24548, 24557, 24672, 24707, 24825, 24850, 25117, 25240, 25454, 25785, 25812, 26491, 26513, 26630, 26991, 27378, 27502, 28024, 28026, 28089, 28113, 28184, 28392, 28466, 28629, 29170, 29444, 29510, 29985, 30260, 30382, 30766, 30795, 30930, 31212, 31315, 32733, 32797, 33103, 33493, 33749, 33915, 34395, 34427, 34516, 34528, 34655, 34743, 35040, 35308, 35410, 35566, 36015, 36305, 36687, 37329, 37392, 37455, 37564, 38250, 38271, 38627, 38730, 39240, 39355, 39409, 39730, 40454, 40525, 40634, 40899, 41503, 41727, 41933, 42009, 42138, 42274, 42334, 42461, 42656, 42991, 43128, 43233, 43484, 43697, 44051, 44502, 44844, 44870, 45019, 45232, 45446, 45551, 45951, 46031, 46155, 46160, 46608, 46627, 46685, 46820, 47166, 47308, 47407, 47485, 47696, 47769, 48290, 48385, 48839, 48851, 48881, 49201, 49736, 49753, 49886, 50037, 50888, 51073, 51650, 52067, 52580, 52690, 52851, 53597, 54045, 54223, 54447, 54470, 54517, 54945, 54957, 55007, 55043, 55047, 55126, 55255, 55256, 55393, 55412, 55911, 56108, 56437, 56635, 56777, 56825, 56927, 57236, 57331, 57364, 57482, 57786, 57909, 57926, 57935, 58013, 58188, 59044, 59283, 59958, 59973, 60106, 60275, 60578, 60727, 60922, 61380, 62176, 62188, 62261, 62458, 62719, 62817, 63077, 63268, 63698, 63715, 63913, 64352, 64409, 64444, 64454, 64556, 64626, 65344, 65685, 65758, 66123, 66315, 66657, 66740, 67137, 67368, 67421, 67470, 67658, 67947, 68021, 68067, 68455, 68630, 68665, 68742, 68998, 69137, 69265, 69549],inplace=True)

#outliers are stored 
outliers = data[(data['ap_hi'] < low_lim) | (data['ap_hi'] > up_lim)]

data = data[(data['ap_hi'] >= low_lim) & (data['ap_hi'] <= up_lim)]

for i in [ 'ap_hi']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""# **Diastolic BP**"""

Q1 = np.percentile(data['ap_lo'],25,interpolation ='midpoint')
Q2 = np.percentile(data['ap_lo'],50,interpolation ='midpoint')
Q3 = np.percentile(data['ap_lo'],75,interpolation ='midpoint')

print(Q1)
print(Q2)
print(Q3)

IQR = Q3-Q1
print(IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR

low_lim

up_lim

#notrequired
outlier=[]
for x in data['ap_lo']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)

#not required
ind1 = data['ap_lo']<low_lim
data.loc[ind1].index

#not required
ind1 = data['ap_lo'] < low_lim
result = data.loc[ind1].index.tolist()

result

outliers = data[(data['ap_lo'] < low_lim) | (data['ap_lo'] > up_lim)]

data = data[(data['ap_lo'] >= low_lim) & (data['ap_lo'] <= up_lim)]

for i in [ 'ap_lo']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

plt.figure(figsize=(6,4))
plt.scatter(data['ap_hi'],data['ap_lo'],s=5)
plt.title('Plot of Systolic BP vs Diastolic BP',fontsize=16)
plt.xlabel('ap_lo')
plt.ylabel('ap_hi')
plt.show()

#  columns for the heatmap
heatmap_cols = ['age', 'height', 'weight', 'ap_lo', 'ap_hi', 'cardio']
heatmap_data = data[heatmap_cols]
# Create heatmap using seaborn
sns.heatmap(heatmap_data.corr(), cmap='coolwarm', annot=True)
plt.title('Correlation Heatmap')
plt.show()

"""# **Weight**"""

Q1 = np.percentile(data['weight'],25,interpolation ='midpoint')
Q2 = np.percentile(data['weight'],50,interpolation ='midpoint')
Q3 = np.percentile(data['weight'],75,interpolation ='midpoint')

print(Q1)
print(Q2)
print(Q3)


IQR = Q3-Q1
print(IQR)

low_lim = (Q1 - 1.5 * IQR)
up_lim = Q3 + 1.5 * IQR

low_lim

up_lim

outlier=[]
for x in data['weight']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)

outlier

ind1 = data['weight'] < low_lim
result = data.loc[ind1].index.tolist()

result

data = data[(data['weight'] >= low_lim) & (data['weight'] <= up_lim)]

for i in [ 'weight']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""# **Height**"""

Q1 = np.percentile(data['height'],25,interpolation ='midpoint')
Q2 = np.percentile(data['height'],50,interpolation ='midpoint')
Q3 = np.percentile(data['height'],75,interpolation ='midpoint')

print(Q1)
print(Q2)
print(Q3)


IQR = Q3-Q1
print(IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR

low_lim

up_lim

outlier=[]
for x in data['height']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)

outlier

ind1 = data['height'] < low_lim
result = data.loc[ind1].index.tolist()

data = data[(data['height'] >= low_lim) & (data['height'] <= up_lim)]

for i in [ 'height']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

"""#**Age**"""

Q1 = np.percentile(data['age'],25,interpolation ='midpoint')
Q2 = np.percentile(data['age'],50,interpolation ='midpoint')
Q3 = np.percentile(data['age'],75,interpolation ='midpoint')

print(Q1)
print(Q2)
print(Q3)


IQR = Q3-Q1
print(IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR

low_lim

up_lim

outlier=[]
for x in data['age']:
   if((x> up_lim) or (x < low_lim)):
       outlier.append(x)

outlier

ind1 = data['age'] < low_lim
result = data.loc[ind1].index.tolist()

data = data[(data['age'] >= low_lim) & (data['age'] <= up_lim)]

for i in [ 'age']:
   plt.figure()
   plt.boxplot(data[i])
   plt.title(i)

#  columns for the heatmap
heatmap_cols = ['age', 'height', 'weight', 'ap_lo', 'ap_hi', 'cardio']
heatmap_data = data[heatmap_cols]
# Create heatmap using seaborn
sns.heatmap(heatmap_data.corr(), cmap='coolwarm', annot=True)
plt.title('Correlation Heatmap')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['height'],data['weight'],s=10)
plt.title('Plot of Height vs Weight',fontsize=16)
plt.xlabel('height')
plt.ylabel('weight')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['gender'],data['ap_hi'],s=10)
plt.title('Plot of Gender vs Systolic blood pressure',fontsize=16)
plt.xlabel('gender')
plt.ylabel('ap_hi')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['gender'],data['ap_lo'],s=10)
plt.title('Plot of Gender vs Diastolic blood pressure',fontsize=16)
plt.xlabel('gender')
plt.ylabel('ap_lo')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['gender'],data['height'],s=10)
plt.title('Plot of Gender vs Height',fontsize=16)
plt.xlabel('gender')
plt.ylabel('height')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['gender'],data['weight'],s=10)
plt.title('Plot of Gender vs Weight',fontsize=16)
plt.xlabel('gender')
plt.ylabel('weight')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['age'],data['height'],s=10)
plt.title('Plot of Age vs Height',fontsize=16)
plt.xlabel('age')
plt.ylabel('height')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['age'],data['weight'],s=10)
plt.title('Plot of Age vs Weight',fontsize=16)
plt.xlabel('age')
plt.ylabel('weight')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['age'],data['ap_hi'],s=10)
plt.title('Plot of Age vs Systolic BP',fontsize=16)
plt.xlabel('age')
plt.ylabel('ap_hi')
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(data['age'],data['ap_lo'],s=10)
plt.title('Plot of Age vs Diastolic BP',fontsize=16)
plt.xlabel('age')
plt.ylabel('ap_lo')
plt.show()

"""\*   After outlier removal the shape of the data is :



"""

data.shape

"""# **Scaling**"""

from sklearn.preprocessing import MinMaxScaler

# Create an instance of the MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

# Fit the scaler to the data and transform the features
scaled_data = scaler.fit_transform(data)

# The scaled_data variable now contains the scaled features

data.head(20)

scaled_data

#  columns for the heatmap
heatmap_cols = ['age', 'height', 'weight', 'ap_lo', 'ap_hi','cholesterol','smoke','gluc','cardio']
heatmap_data = data[heatmap_cols]
# Create heatmap using seaborn
plt.figure(figsize=(10, 6)) 
sns.heatmap(heatmap_data.corr(), cmap='coolwarm', annot=True)
plt.title('Correlation Heatmap')
plt.show()

X=data.drop('cardio',axis=1)
y=data['cardio']

X

y



"""### **Linear Regression**"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score,f1_score, precision_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
threshold = 0.5
y_pred_binary = [1 if val >= threshold else 0 for val in y_pred]

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
 #Calculate F1 score
f1 = f1_score(y_test, y_pred_binary)

# Calculate precision
precision = precision_score(y_test, y_pred_binary)

print('Mean squared error is: ',mse)
print('R Squared value is: ',r2)
print("F1 Score:", f1)
print("Precision:", precision)

"""### **Decision Tree**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.metrics import mean_squared_error, r2_score,f1_score, precision_score
dt_clf=DecisionTreeClassifier()

dt_clf.fit(X_train,y_train)
y_pred=dt_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
# Calculate F1 score
f1 = f1_score(y_test, y_pred)

# Calculate precision
precision = precision_score(y_test, y_pred)

print('Mean squared error is: ',mse)
print('R Squared value is: ',r2)

print("F1 Score:", f1)
print("Precision:", precision)

print('Accuracy is: ',accuracy)
print('Confusion matrix : ',confusion_matrix(y_test,y_pred))

"""### **Random Forest model**"""

from sklearn.ensemble import RandomForestClassifier  # For classification tasks
from sklearn.ensemble import RandomForestRegressor  # For regression tasks
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score  # For classification tasks
from sklearn.metrics import mean_squared_error  # For regression tasks

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
# For regression tasks, use RandomForestRegressor instead

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print('Accuracy : ',accuracy)
print ('Mean-Squared-error :',mse)

"""### **Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
logic_m=LogisticRegression()
logic_m.fit(X_train,y_train)
y_pred=logic_m.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('Mean squared error is: ',mse)
print('R Squared value is: ',r2)

from sklearn.metrics import accuracy_score,confusion_matrix
print("Accuracy is : ",accuracy_score(y_test,y_pred))
print('Confusion matrix : ',confusion_matrix(y_test,y_pred))

"""### **SVM Model**

### **KNeighbors**
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

#Define a range of k values to test
#k_value = range(1, 100)  # Test k values from 1 to 100                  

# Create an empty list to store the cross-validation scores
#cv_s = []

#for k in k_value:
    #knn = KNeighborsClassifier(n_neighbors=k)
    #scores = cross_val_score(knn, X_train, y_train, cv=5)  
    #cv_s.append(np.mean(scores))

# Print the validation scores for each k value
#for k, score in zip(k_value, cv_s):
    #print("K =", k, "  Validation Score =", score)

k = 9  # Set the number of neighbors
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print('Confusion matrix : ',confusion_matrix(y_test,y_pred))

"""### **Naive Bayes model**"""

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score,confusion_matrix

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print('Mean squared error is: ',mse)
print('R Squared value is: ',r2)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print('Confusion matrix : ',confusion_matrix(y_test,y_pred))

"""### **XgBoost**"""

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

"""### **Gradient Boosting**"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score

# Load your data and split it into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Gradient Boosting Classifier
gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Train the GBM model
gbm.fit(X_train, y_train)

# Predict on the test set
y_pred = gbm.predict(X_test)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Precision: {:.2f}".format(precision))
print("Recall: {:.2f}".format(recall))
print("F1 Score: {:.2f}".format(f1))

"""### Target 3

### **Hyper parameter tuning**

### Using Grid SearchCV
"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier


# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a gradient boosting classifier
gb_classifier = GradientBoostingClassifier()

# Define the hyperparameters grid for tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.1, 0.05, 0.01],
    'max_depth': [3, 4, 5]
}

# Perform grid search cross-validation
grid_search = GridSearchCV(gb_classifier, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and the corresponding accuracy score
print("Best Hyperparameters: ", grid_search.best_params_)
print("Best Accuracy Score: ", grid_search.best_score_)

# Evaluate the model on the test set
best_model = grid_search.best_estimator_
test_accuracy = best_model.score(X_test, y_test)
print("Test Accuracy: ", test_accuracy)

#from sklearn.ensemble import GradientBoostingClassifier
#from sklearn.model_selection import RandomizedSearchCV
#from sklearn.datasets import make_classification
#from sklearn.model_selection import train_test_split
#from scipy.stats import uniform, randint

# Generate a sample dataset for demonstration
#X, y = make_classification(random_state=42)

# Split the dataset into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a gradient boosting classifier
#clf = GradientBoostingClassifier()

# Define the hyperparameter distributions
#param_dist = {
 #   'n_estimators': randint(100, 1000),  # Number of trees in the gradient boosting ensemble
  ## 'max_depth': randint(3, 10),  # Maximum depth of each tree
#}

# Perform randomized search cross-validation
#random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=10, cv=5)
#random_search.fit(X_train, y_train)

# Print the best hyperparameters
#print("Best Hyperparameters: ", random_search.best_params_)

# Evaluate the performance on the test set using the best model
#best_model = random_search.best_estimator_
#accuracy = best_model.score(X_test, y_test)
#print("Accuracy: ", accuracy)

import pickle

# Make pickle file of our model
pickle.dump(gb_classifier, open("model.pkl", "wb"))